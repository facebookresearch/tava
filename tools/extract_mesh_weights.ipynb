{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from hydra import initialize, compose\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import OmegaConf\n",
    "from tava.utils.training import resume_from_ckpt\n",
    "from tava.models.basic.mipnerf import cylinder_to_gaussian\n",
    "\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args for data\n",
    "ARGS_ANIMAL_WOLF = [\n",
    "    \"dataset=animal_wolf\", \"dataset.root_fp=/home/ruilongli/data/forest_and_friends_rendering/\",\n",
    "    \"hydra.run.dir=/home/ruilongli/workspace/TAVA/outputs/release/animal_wolf/Wolf_cub_full_RM_2/snarf/\",\n",
    "]\n",
    "ARGS_ANIMAL_HARE=[\n",
    "    \"dataset=animal_hare\", \"dataset.root_fp=/home/ruilongli/data/forest_and_friends_rendering/\",\n",
    "    \"hydra.run.dir=/home/ruilongli/workspace/TAVA/outputs/release/animal_hare/Hare_male_full_RM/snarf/\",\n",
    "]\n",
    "ARGS_ZJU_313=[\n",
    "    \"dataset=zju\", \"dataset.subject_id=313\", \"dataset.root_fp=/home/ruilongli/data/zju_mocap/neuralbody/\",\n",
    "    \"hydra.run.dir=/home/ruilongli/workspace/TAVA/outputs/release/zju_mocap/313/snarf/\",\n",
    "]\n",
    "ARGS_ZJU_315=[\n",
    "    \"dataset=zju\", \"dataset.subject_id=315\", \"dataset.root_fp=/home/ruilongli/data/zju_mocap/neuralbody/\",\n",
    "    \"hydra.run.dir=/home/ruilongli/workspace/TAVA/outputs/release/zju_mocap/315/snarf/\",\n",
    "]\n",
    "\n",
    "# args for method\n",
    "ARGS_TAVA_ANIMAL=[\"pos_enc=snarf\", \"loss_bone_w_mult=1.0\", \"pos_enc.offset_net_enabled=false\", \"model.shading_mode=null\"]\n",
    "ARGS_TAVA_ZJU=[\"pos_enc=snarf\", \"loss_bone_w_mult=1.0\", \"pos_enc.offset_net_enabled=true\", \"model.shading_mode=implicit_AO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we set the arguments for ZJU_313 as an example.\n",
    "overrides = [\"resume=True\"] + ARGS_ANIMAL_HARE + ARGS_TAVA_ANIMAL\n",
    "\n",
    "# create the cfg\n",
    "with initialize(config_path=\"../configs\"):\n",
    "    cfg = compose(config_name=\"mipnerf_dyn\", overrides=overrides, return_hydra_config=True)\n",
    "    OmegaConf.resolve(cfg.hydra)\n",
    "    save_dir = cfg.hydra.run.dir\n",
    "    ckpt_dir = os.path.join(save_dir, \"checkpoints\")\n",
    "    \n",
    "# initialize model and load ckpt\n",
    "model = instantiate(cfg.model).to(device)\n",
    "_ = resume_from_ckpt(\n",
    "    path=ckpt_dir, model=model, step=cfg.resume_step, strict=True,\n",
    ")\n",
    "assert os.path.exists(ckpt_dir), \"ckpt does not exist! Please check.\"\n",
    "\n",
    "# initialize dataset\n",
    "dataset = instantiate(\n",
    "    cfg.dataset, split=\"train\", num_rays=None, cache_n_repeat=None,\n",
    ")\n",
    "meta_data = dataset.build_pose_meta_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a color pattern for visualization\n",
    "torch.manual_seed(412)\n",
    "colorbases = torch.rand((cfg.dataset.n_transforms + 1, 3)) * 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the Mip-NeRF requires a covariance for density & color querying,\n",
    "# we here *estimate* a `cov` based on the size of the subject and the \n",
    "# number of sampled points. It can be estimated in other ways such as\n",
    "# average of cov during training.\n",
    "\n",
    "radii = dataset[0][\"rays\"].radii.mean().to(device)\n",
    "if isinstance(cfg.dataset.subject_id, str):  # animal\n",
    "    rest_verts = torch.from_numpy(\n",
    "        dataset.parser.load_meta_data(dataset.parser.actions[0])[\"rest_verts\"]\n",
    "    ).to(device)\n",
    "else:\n",
    "    rest_verts = torch.from_numpy(\n",
    "        dataset.parser.load_meta_data()[\"rest_verts\"]\n",
    "    ).to(device)\n",
    "bboxs_min = rest_verts.min(dim=0).values\n",
    "bboxs_max = rest_verts.max(dim=0).values\n",
    "subject_size = torch.prod(bboxs_max - bboxs_min) ** (1./3.)\n",
    "t0, t1 = 0, subject_size / model.num_samples\n",
    "d = torch.tensor([0., 0., 1.]).to(device)\n",
    "_, cov = cylinder_to_gaussian(d, t0, t1, radii, model.diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid coordinates to be queried in canonical.\n",
    "\n",
    "def create_grid3D(min, max, steps, device=\"cpu\"):\n",
    "    if type(min) is int:\n",
    "        min = (min, min, min) # (x, y, z)\n",
    "    if type(max) is int:\n",
    "        max = (max, max, max) # (x, y)\n",
    "    if type(steps) is int:\n",
    "        steps = (steps, steps, steps) # (x, y, z)\n",
    "    arrangeX = torch.linspace(min[0], max[0], steps[0]).to(device)\n",
    "    arrangeY = torch.linspace(min[1], max[1], steps[1]).to(device)\n",
    "    arrangeZ = torch.linspace(min[2], max[2], steps[2]).to(device)\n",
    "    gridX, girdY, gridZ = torch.meshgrid([arrangeX, arrangeY, arrangeZ], indexing=\"ij\")\n",
    "    coords = torch.stack([gridX, girdY, gridZ]) # [3, steps[0], steps[1], steps[2]]\n",
    "    coords = coords.view(3, -1).t() # [N, 3]\n",
    "    return coords\n",
    "_center = (bboxs_max + bboxs_min) / 2.0\n",
    "_scale = (bboxs_max - bboxs_min) / 2.0\n",
    "bboxs_min_large = _center - _scale * 1.5\n",
    "bboxs_max_large = _center + _scale * 1.5\n",
    "res = 512\n",
    "coords = create_grid3D(bboxs_min_large, bboxs_max_large, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the density grid in the canonical\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tava.utils.bone import closest_distance_to_points\n",
    "from tava.utils.structures import namedtuple_map\n",
    "\n",
    "bones_rest = namedtuple_map(lambda x: x.to(device), meta_data[\"bones_rest\"])\n",
    "chunk_size = 20480\n",
    "densities = []\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, coords.shape[0], chunk_size)):\n",
    "        coords_chunk = coords[i: i + chunk_size].to(device)\n",
    "        \n",
    "        dists = closest_distance_to_points(bones_rest, coords_chunk).min(dim=-1).values\n",
    "        selector = dists <= cfg.dataset.cano_dist\n",
    "\n",
    "        posi_enc = model.pos_enc.posi_enc((coords_chunk, cov.expand_as(coords_chunk)))\n",
    "        sigma = model.mlp.query_sigma(posi_enc, masks=selector, return_feat=False)\n",
    "        density = model.density_activation(sigma + model.density_bias)\n",
    "        densities.append(density.cpu())\n",
    "densities = torch.cat(densities, dim=0).reshape(res, res, res)\n",
    "print (densities.min(), densities.max(), densities.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marching cube to get the mesh. We use the threshold 5.0 for all cases. You might want\n",
    "# to adjust that with your own data. Note installing torchmcubes would take some time.\n",
    "\n",
    "# excute this in juputer to install: \"!pip install git+https://github.com/tatsy/torchmcubes.git\"\n",
    "from torchmcubes import marching_cubes\n",
    "\n",
    "verts, faces = marching_cubes(densities, 5.0)\n",
    "verts = verts[..., [2, 1, 0]] / res * (bboxs_max_large - bboxs_min_large).cpu() + bboxs_min_large.cpu()\n",
    "print (verts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Mesh with Skinning Weights\n",
    "\n",
    "def save_obj_mesh(mesh_path, verts, faces):\n",
    "    file = open(mesh_path, 'w')\n",
    "    for v in verts:\n",
    "        file.write('v %.4f %.4f %.4f\\n' % (v[0], v[1], v[2]))\n",
    "    for f in faces:\n",
    "        f_plus = f + 1\n",
    "        file.write('f %d %d %d\\n' % (f_plus[0], f_plus[1], f_plus[2]))\n",
    "    file.close()\n",
    "\n",
    "def save_obj_mesh_with_color(mesh_path, verts, faces, colors):\n",
    "    file = open(mesh_path, 'w')\n",
    "\n",
    "    for idx, v in enumerate(verts):\n",
    "        c = colors[idx]\n",
    "        file.write('v %.4f %.4f %.4f %.4f %.4f %.4f\\n' % (v[0], v[1], v[2], c[0], c[1], c[2]))\n",
    "    for f in faces:\n",
    "        f_plus = f + 1\n",
    "        file.write('f %d %d %d\\n' % (f_plus[0], f_plus[1], f_plus[2]))\n",
    "    file.close()\n",
    "\n",
    "chunk_size = 8192\n",
    "weights = []\n",
    "with torch.no_grad():\n",
    "    for i in range(0, verts.shape[0], chunk_size):\n",
    "        verts_chunk = verts[i: i + chunk_size].to(device)\n",
    "        weights_chunk = model.pos_enc.query_weights(verts_chunk)\n",
    "        weights.append(weights_chunk.cpu())\n",
    "weights = torch.cat(weights, dim=0)\n",
    "\n",
    "colors = (weights[..., None] * colorbases).sum(dim=-2)\n",
    "save_obj_mesh_with_color(\n",
    "    os.path.join(save_dir, \"extracted_cano_mesh_soft_weight.obj\"), \n",
    "    verts.cpu().numpy(), faces.cpu().numpy(), colors.cpu().numpy() / 255.0\n",
    ")\n",
    "\n",
    "colors = colorbases[weights.argmax(dim=-1)]\n",
    "save_obj_mesh_with_color(\n",
    "    os.path.join(save_dir, \"extracted_cano_mesh_hard_weight.obj\"), \n",
    "    verts.cpu().numpy(), faces.cpu().numpy(), colors.cpu().numpy() / 255.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f156ac58ad89dad164aee97f58386e335ecc1447b9ed93fd42b558e9d07d43f1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('tava')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
